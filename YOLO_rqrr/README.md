# YOLO_rqrr â€” Quick and dirty WASM QR Detection Experiment

## Notice
While the results of the experiment might be valuable in informing a proper project, this is beyond my capacity at this time. It is provided as is without warrenty or guarentee of reproducibility.

> **YOLOv12n** (attention-centric object detector) â†’ **ZXing** (C++/WASM barcode decoder) or rqrr (rust/WASM QR decoder)
> Tested and trained against Kipukas' anti-cheat camouflaged QR codes on mobile browsers.

## Experiment Summary

This directory contains the complete experiment exploring whether YOLOv12's
attention mechanism, trained on a custom QR code dataset, could improve QR
detection speed and accuracy for Kipukas' camouflaged QR codes on low-res
front-facing cameras. This was tested on google pixel 3a, google pixel 4a, google pixel 9, iphone 6s, iphone 14 pro, samsung galaxy s21, and samsung galaxy book 3. YOLOv12n displays good tracking on all capable devices. General detection was best with ZXing compiled to WASM in good environmental conditions. Additionally, it is compatible with the older devices and devices which do not yet support WebGPU. In poor conditions, YOLOv12n driven cropping + adaptive threshold preprocessing yelds positive detection results with both ZXing and rqrr backends. While not as performant as ZXing, YOLOv12n + rqrr + at_21 was able to decode reliably and would be judged sufficient for more standard QR workflows. However, its main benefit (small compared to ZXing) is deminished when paired with YOLOv12n (~10x larger than ZXing).

### Findings

| Approach | Result |
|----------|--------|
| **rqrr alone** (28 preprocessing strategies on full frame) | âš ï¸ very slow w. adaptive threashold, Fails wo.â€” finder pattern detection can't see through SVG camouflage |
| **ZXing-only (std-WASM/CDN)** (no YOLO, full-frame scan) | âš ï¸ very slow/fails to detect on most devices |
| **YOLOv12n + rqrr** | âš ï¸ slow but functional on powerful devices |
| **YOLO v12n + ZXing** (two-stage: detect â†’ crop â†’ decode) | âœ… Works â€” YOLO learns camouflage patterns, ZXing decodes clean crops, bogs older devices |
| **YOLO on WASM/CPU** | âš ï¸ Slow but functional on powerful devices (laptops) |
| **YOLO on WebGPU** | âœ… Fast on supported mobile GPUs |
| **ZXing-only (gcc17, compiled in house)** (no YOLO, full-frame scan) | âœ… Works very fast with close shots and good environment on all devices |

### Key Decisions

1. **User-controlled CV toggle** â€” Auto-detecting WebGPU capability is unreliable.
   Some older devices report WebGPU but perform poorly; some without WebGPU have
   CPUs powerful enough for WASM inference. The chip icon (â¬œ off / ğŸŸ© on) in the
   scanner UI lets users opt in to YOLO. Default: ZXing-only.

2. **ZXing replaced rqrr for decode** â€” rqrr is a Rust QR decoder, but ZXing
   (C++/WASM) with `tryHarder` mode proved more reliable for decoded crops and has
   a mature WASM distribution. rqrr remains in the repo for reference.

3. **Square 640Ã—640 capture** â€” Camera canvas matches YOLO's native input resolution.
   No letterbox distortion, what the user sees is exactly what the decoder receives.

4. **Otsu removed from augmentation** â€” Otsu's global threshold washes out the
   reflective surfaces on physical cards, producing all-white training images.
   9 transforms remain (adaptive_thresh Ã—3, CLAHE, blur+AT, contrast_stretch+AT,
   yellow-aware, gaussian noise, JPEG compression).

5. **5-second eager preload** â€” ONNX model + ZXing WASM load asynchronously 5s
   after page load, so the scanner feels instant when opened.

## Architecture

```
Camera Frame (640Ã—640 RGBA, 1:1 square)
    â”‚
    â”œâ”€â”€â”€ CV OFF (default) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                            â–¼
    â”‚                                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                   â”‚  ZXing Decode    â”‚
    â”‚                                   â”‚  Full-frame scan â”‚
    â”‚                                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚                                            â”‚
    â”œâ”€â”€â”€ CV ON (user toggle) â”€â”€â”€â”                â”‚
    â”‚                           â–¼                â”‚
    â”‚               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
    â”‚               â”‚  Stage 1: YOLOv12n    â”‚    â”‚
    â”‚               â”‚  ONNX Runtime Web     â”‚    â”‚
    â”‚               â”‚  (WebGPU â†’ WASM)      â”‚    â”‚
    â”‚               â”‚  ~30-80ms (GPU)       â”‚    â”‚
    â”‚               â”‚  ~1-4s (CPU)          â”‚    â”‚
    â”‚               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
    â”‚                           â”‚ bbox crop      â”‚
    â”‚                           â–¼                â”‚
    â”‚               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
    â”‚               â”‚  Stage 2: ZXing       â”‚    â”‚
    â”‚               â”‚  Decode cropped ROI   â”‚    â”‚
    â”‚               â”‚  tryHarder mode       â”‚    â”‚
    â”‚               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
    â”‚                           â”‚                â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
                    decoded URL â†’ WASM server
                    â†’ validation â†’ redirect
```

## Why Two Stages?

The single-stage approach (rqrr with preprocessing on full frames) fails because
**rqrr/ZXing finder pattern detection can't locate QR codes through Kipukas'
cracked-lava SVG camouflage texture**. No amount of image preprocessing fixes a
decoder that can't see the three position squares in a noisy full-resolution frame.

YOLOv12n **learns** what camouflaged QR codes look like. Its Area Attention
mechanism gives it a global receptive field â€” it understands the whole region
contextually, not just edges and corners. Once YOLO provides a tight bounding
box, ZXing gets a clean, high-effective-resolution crop where decode becomes
highly reliable.

## Training Pipeline

### Dataset

- **70 Kipukas captures** (`kipukas-qr-dataset-70imgs/`) â€” annotated in the
  custom annotator (`annotator/index.html`), captured at 1280Ã—720 from the
  scanner's front-facing camera with real printed camouflaged cards
- **kolabit public dataset** (`data/`) â€” ~600 general QR code images for
  diversity, fetched via `train/fetch_dataset.py`
- **9 augmentation transforms** applied to Kipukas images via `train/augment.py`:
  1. `at15` â€” adaptive threshold (block=15, c=8)
  2. `at11` â€” adaptive threshold fine (block=11, c=6)
  3. `at21` â€” adaptive threshold coarse (block=21, c=10)
  4. `clahe` â€” CLAHE (4Ã—4 tiles, clip=2.0)
  5. `blur_at` â€” Gaussian blur + adaptive threshold
  6. `stretch_at` â€” contrast stretch + adaptive threshold
  7. `yellow` â€” yellow-aware channel (max(R,G) - B) for anti-camouflage
  8. `noise15` â€” Gaussian noise (Ïƒ=15)
  9. `jpeg35` â€” JPEG compression artifacts (quality=35)

### Training

```bash
cd YOLO_rqrr

# 1. Augment local QR + merge with kolabit
uv run python train/augment.py

# 2. Train YOLOv12n (100 epochs, MPS on Apple Silicon)
uv run python train/train.py --epochs 100 --device mps

# 3. Export to ONNX (opset 12, WebGPU compatible)
uv run python train/export_onnx.py --weights /Users/lah-rb/Repos/lah-rb.github.io/runs/detect/runs/detect/train/weights/best.pt

# 4. Copy model to site assets
cp models/yolo12n-qr.onnx ../assets/js-wasm/yolo12n-qr.onnx
```

### Validation

```bash
uv run python train/validate.py
```

## Runtime Integration

### Files in the main site

| File | Purpose |
|------|---------|
| `assets/js/yolo-inference.js` | ONNX Runtime Web session (WebGPU â†’ WASM fallback) |
| `assets/js/postprocess.js` | YOLO output â†’ bboxes (NMS, confidence threshold) |
| `assets/js/zxing-decode.js` | ZXing C++/WASM barcode decoder |
| `assets/js/kipukas-worker.js` | Web Worker orchestrating YOLO+ZXing or ZXing-only |
| `assets/js/kipukas-api.js` | 5s delayed PRELOAD_QR, CV preference relay |
| `assets/js/qr-camera.js` | Camera capture, frame relay, bbox overlay |
| `kipukas-server/src/routes/qr.rs` | Scanner UI HTML (Rust/WASM), CV toggle button |
| `assets/js-wasm/yolo12n-qr.onnx` | Exported YOLO model (~5MB) |

### CV Toggle Flow

```
User taps chip icon in scanner UI
    â†’ Alpine toggles cvOn state
    â†’ localStorage.setItem('kipukas-cv-enabled', true/false)
    â†’ kipukasWorker.postMessage({ type: 'SET_CV_MODE', enabled })
    â†’ Worker updates qrMode:
        ON:  resets qrReady, next frame triggers YOLO init
        OFF: switches to 'zxing-only' (YOLO session stays loaded but unused)
```

### Preload Flow

```
Page loads â†’ kipukas-api.js spawns worker
    â†’ 5s timeout fires
    â†’ Reads localStorage('kipukas-cv-enabled')
    â†’ Sends PRELOAD_QR { cvEnabled } to worker
    â†’ Worker inits:
        cvEnabled=true:  YOLO (WebGPUâ†’WASM) + ZXing in parallel
        cvEnabled=false: ZXing only
```

## Project Structure

```
YOLO_rqrr/
â”œâ”€â”€ README.md                  # This file
â”œâ”€â”€ pyproject.toml             # Python project config (uv)
â”œâ”€â”€ .python-version
â”‚
â”œâ”€â”€ train/                     # Python â€” Training pipeline
â”‚   â”œâ”€â”€ augment.py             # Kipukas augmentation + dataset merge
â”‚   â”œâ”€â”€ train.py               # Fine-tune YOLOv12n
â”‚   â”œâ”€â”€ export_onnx.py         # Export â†’ ONNX (opset 12)
â”‚   â”œâ”€â”€ validate.py            # Evaluate model performance
â”‚   â”œâ”€â”€ fetch_dataset.py       # Download kolabit public dataset
â”‚   â”œâ”€â”€ dataset.yaml           # Auto-generated dataset config
â”‚   â””â”€â”€ requirements.txt       # ultralytics, torch, onnx
â”‚
â”œâ”€â”€ annotator/                 # Browser-based bbox annotation tool
â”‚   â””â”€â”€ index.html             # Capture + annotate QR bounding boxes
â”‚
â”œâ”€â”€ kipukas-qr-dataset-70imgs/ # Annotated Kipukas captures
â”‚   â”œâ”€â”€ images/train/          # 70 JPG captures from scanner camera
â”‚   â””â”€â”€ labels/train/          # YOLO-format label files
â”‚
â”œâ”€â”€ data/                      # kolabit dataset (gitignored)
â”œâ”€â”€ data-augmented/            # Merged augmented dataset (gitignored)
â”‚
â”œâ”€â”€ rqrr-wasm/                 # Rust QR decode WASM crate (reference)
â”‚   â”œâ”€â”€ Cargo.toml
â”‚   â””â”€â”€ src/lib.rs
â”‚
â”œâ”€â”€ web/                       # Standalone test harness
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ yolo-inference.js
â”‚   â”‚   â”œâ”€â”€ postprocess.js
â”‚   â”‚   â””â”€â”€ yolo-rqrr-worker.js
â”‚   â””â”€â”€ index.html
â”‚
â”œâ”€â”€ models/                    # Exported models (gitignored)
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ build-rqrr-wasm.sh
â”‚   â””â”€â”€ integrate.sh
â”‚
â”œâ”€â”€ train_100epoch.log         # Training logs
â”œâ”€â”€ train_320_fp16.log
â””â”€â”€ train_augmented.log
```

## License

- **YOLOv12**: AGPL-3.0 (Ultralytics) â€” training pipeline and exported model
- **ZXing-cpp**: Apache-2.0
- **rqrr**: MIT/Apache-2.0
- **ONNX Runtime Web**: MIT
- **This integration code**: AGPL-3.0 (to satisfy YOLO's copyleft)

Per AGPL requirements, the complete QR detection component is published in
this public repository alongside the Kipukas production site.
